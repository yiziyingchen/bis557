library(bis557)
fit <- linear_model(Sepal.Length ~ ., iris)
summary(fit)
summary(fit)
library(bis557)
fit <- linear_model(Sepal.Length ~ ., iris)
fit
summary(fit)
fit
install.packages(devtools)
install.packages('devtools')
library(devtools)
getwd()
setwd("/Users/yiziyingchen/Desktop/bis557")
check()
installed.packages()
check()
check()
check()
data("emnist_test")
data("emnist_train")
y_test = as.matrix(emnist_test$`1`)
test()
install.packages("keras")
test()
library(bis557)
test()
test()
check()
test()
# Inputs: y, response vector; a, predicted
# response
# Output: gradient of the absolute deviation loss function
abs_p <- function(y, a) {
if(a<y){
der = -1
}
if(a>=y){
der = 1
}
der
}
# Inputs: v, a vector, matrix or array
# Output: input with negative terms pushed to zero
ReLU <- function(v) {
v[v < 0] <- 0
v
}
# Inputs: v, a vector, matrix or array
# Output: converts positive terms to one and
# non-positive terms to zero.
ReLU_p <- function(v) {
p <- v * 0
p[v > 0] <- 1
p
}
# Inputs: sizes, a vector giving the size of
# layer in the neural network, including
# the input and output layers
# Output: a list containing initialized weights,
# biases, and momentums
make_weights <- function(sizes) {
L <- length(sizes) - 1
weights <- vector("list", L)
for (j in seq_len(L)) {
w <- matrix(rnorm(sizes[j] * sizes[j + 1],
sd = 1/sqrt(sizes[j])),
ncol = sizes[j],
nrow = sizes[j + 1])
v <- matrix(0,
ncol = sizes[j],
nrow = sizes[j + 1])
weights[[j]] <- list(w = w,
v = v,
b = rnorm(sizes[j + 1]))
}
weights
}
f_prop <- function(x, weights, sigma) {
L <- length(weights)
z <- vector("list", L)
a <- vector("list", L)
for (j in seq_len(L)) {
a_j1 <- if(j == 1) x else a[[j - 1L]]
z[[j]] <- weights[[j]]$w %*% a_j1 + weights[[j]]$b
a[[j]] <- if (j != L) sigma(z[[j]]) else z[[j]]
}
return(list(z = z, a = a))
}
b_prop <- function(x, y, weights, f_obj, sigma_p, f_p) {
z <- f_obj$z; a <- f_obj$a
L <- length(weights)
grad_z <- vector("list", L)
grad_w <- vector("list", L)
for (j in rev(seq_len(L))) {
if (j == L) {
grad_z[[j]] <- f_p(y, a[[j]])
} else {
grad_z[[j]] <- (t(weights[[j + 1]]$w) %*%
grad_z[[j + 1]]) * sigma_p(z[[j]])
}
a_j1 <- if(j == 1) x else a[[j - 1L]]
grad_w[[j]] <- grad_z[[j]] %*% t(a_j1)
}
return(list(grad_z = grad_z, grad_w = grad_w))
}
# Inputs: X, data matrix; y, response; sizes, vector
# given the number of neurons in each layer of
# the neural network; epochs, integer number of
# epochs to complete; eta, learning rate; mu,
# momentum term; l2, penalty term for l2-norm;
# weights, optional list of starting weights
# Output: the trained weights for the neural network
nn_sgd <- function(X, y, sizes, epochs, eta, mu = 0,
l2 = 0, weights = NULL) {
if (is.null(weights))
weights <- make_weights(sizes)
for (epoch in seq_len(epochs)) {
for (i in seq_len(nrow(X))) {
f_obj <- f_prop(X[i,], weights, ReLU)
b_obj <- b_prop(X[i,], y[i,], weights, f_obj,
ReLU_p, abs_p)
for (j in seq_along(b_obj)) {
weights[[j]]$b <- weights[[j]]$b -
eta * b_obj$grad_z[[j]]
weights[[j]]$v <- mu * weights[[j]]$v -
eta * b_obj$grad_w[[j]]
weights[[j]]$w <- (1 - eta * l2) *
weights[[j]]$w +
weights[[j]]$v
}
}
}
return(weights)
}
l2_norm <- rep(NA_real_, 3)
l2_vals <- c(0, 0.01, 0.04, 0.05)
X <- matrix(runif(1000, min = -1, max = 1), ncol = 1)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
weights_start <- make_weights(sizes = c(1, 10, 1))
for (i in seq_along(l2_vals)) {
weights <- nn_sgd(X, y, weights = weights_start,
epochs = 10, eta = 0.1,
l2 = l2_vals[i])
l2_norm[i] <- sum((weights[[1]]$w)^2)
}
l2_norm
